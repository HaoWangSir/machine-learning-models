{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[simple autoencoder](https://blog.keras.io/building-autoencoders-in-keras.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# intermediate result\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.3525 - val_loss: 0.2694\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.2611 - val_loss: 0.2489\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.2382 - val_loss: 0.2254\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2182 - val_loss: 0.2086\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.2039 - val_loss: 0.1966\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1937 - val_loss: 0.1880\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1857 - val_loss: 0.1807\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1791 - val_loss: 0.1746\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1733 - val_loss: 0.1693\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1684 - val_loss: 0.1647\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1640 - val_loss: 0.1605\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1601 - val_loss: 0.1567\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1564 - val_loss: 0.1531\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1529 - val_loss: 0.1497\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1496 - val_loss: 0.1466\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1465 - val_loss: 0.1437\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1437 - val_loss: 0.1411\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1411 - val_loss: 0.1383\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1386 - val_loss: 0.1360\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1364 - val_loss: 0.1337\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1342 - val_loss: 0.1317\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1322 - val_loss: 0.1295\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1302 - val_loss: 0.1277\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1284 - val_loss: 0.1258\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1266 - val_loss: 0.1241\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1249 - val_loss: 0.1225\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1233 - val_loss: 0.1209\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1218 - val_loss: 0.1194\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1203 - val_loss: 0.1180\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1190 - val_loss: 0.1167\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1177 - val_loss: 0.1154\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1164 - val_loss: 0.1142\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1153 - val_loss: 0.1132\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1142 - val_loss: 0.1121\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1132 - val_loss: 0.1111\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1122 - val_loss: 0.1101\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1113 - val_loss: 0.1093\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1105 - val_loss: 0.1085\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1097 - val_loss: 0.1077\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1089 - val_loss: 0.1070\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1082 - val_loss: 0.1063\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1076 - val_loss: 0.1056\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1070 - val_loss: 0.1051\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1064 - val_loss: 0.1045\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1058 - val_loss: 0.1040\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1053 - val_loss: 0.1035\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1049 - val_loss: 0.1030\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1044 - val_loss: 0.1026\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1040 - val_loss: 0.1022\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1036 - val_loss: 0.1018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb474c4f28>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reconst_test = autoencoder.predict(x_test)\n",
    "encode_test = encoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "row = 2\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(row, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(row, n, i + 1 + n)\n",
    "    plt.imshow(reconst_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_columns = [c for c in train.columns if c not in ['Team1', 'Team2', 'Match', 'Label']]\n",
    "target_column = \"Label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46909, 175)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../Kaggle-NCAA/train.csv\")\n",
    "train.fillna(0, inplace=True)\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    x_max = max(x)\n",
    "    x_min = min(x)\n",
    "    \n",
    "    return (x-x_min)/(x_max-x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in feat_columns:\n",
    "    train[f] = norm(train[f])\n",
    "    \n",
    "X_train, X_val, y_train, y_val = train_test_split(train[feat_columns], train[target_column], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Type</th>\n",
       "      <th>WScore_min_x</th>\n",
       "      <th>WScore_mean_x</th>\n",
       "      <th>WScore_max_x</th>\n",
       "      <th>WFGM_min_x</th>\n",
       "      <th>WFGM_mean_x</th>\n",
       "      <th>WFGM_max_x</th>\n",
       "      <th>WFGA_min_x</th>\n",
       "      <th>WFGA_mean_x</th>\n",
       "      <th>...</th>\n",
       "      <th>LStl_min_y</th>\n",
       "      <th>LStl_mean_y</th>\n",
       "      <th>LStl_max_y</th>\n",
       "      <th>LBlk_min_y</th>\n",
       "      <th>LBlk_mean_y</th>\n",
       "      <th>LBlk_max_y</th>\n",
       "      <th>LPF_min_y</th>\n",
       "      <th>LPF_mean_y</th>\n",
       "      <th>LPF_max_y</th>\n",
       "      <th>seed_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38149</th>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.414480</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.478631</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.520766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.206182</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416293</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.677362</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4125</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.422183</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.451066</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.442308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.503736</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474039</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.619737</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35140</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.377561</td>\n",
       "      <td>0.276923</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.368302</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.506804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.426262</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.345234</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.523626</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23546</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.181802</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.262438</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.503300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295175</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.322139</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.421206</td>\n",
       "      <td>0.292308</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.445052</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.457060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.335268</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255732</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.324756</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Season  Type  WScore_min_x  WScore_mean_x  WScore_max_x  WFGM_min_x  \\\n",
       "38149   0.375   0.0      0.540541       0.414480      0.400000    0.545455   \n",
       "4125    0.250   0.0      0.486486       0.422183      0.353846    0.636364   \n",
       "35140   0.250   0.0      0.621622       0.377561      0.276923    0.818182   \n",
       "23546   0.250   0.0      0.432432       0.181802      0.184615    0.363636   \n",
       "546     0.875   0.0      0.405405       0.421206      0.292308    0.545455   \n",
       "\n",
       "       WFGM_mean_x  WFGM_max_x  WFGA_min_x  WFGA_mean_x    ...      \\\n",
       "38149     0.478631    0.513514    0.409091     0.520766    ...       \n",
       "4125      0.451066    0.432432    0.545455     0.442308    ...       \n",
       "35140     0.368302    0.351351    0.500000     0.506804    ...       \n",
       "23546     0.262438    0.351351    0.500000     0.503300    ...       \n",
       "546       0.445052    0.324324    0.454545     0.457060    ...       \n",
       "\n",
       "       LStl_min_y  LStl_mean_y  LStl_max_y  LBlk_min_y  LBlk_mean_y  \\\n",
       "38149    0.333333     0.206182    0.133333         0.0     0.416293   \n",
       "4125     0.666667     0.503736    0.466667         0.0     0.474039   \n",
       "35140    0.333333     0.426262    0.600000         0.0     0.345234   \n",
       "23546    0.000000     0.295175    0.266667         0.0     0.129896   \n",
       "546      0.666667     0.335268    0.266667         0.0     0.255732   \n",
       "\n",
       "       LBlk_max_y  LPF_min_y  LPF_mean_y  LPF_max_y  seed_diff  \n",
       "38149         0.3   0.545455    0.677362       0.60        0.5  \n",
       "4125          0.5   0.454545    0.619737       0.40        0.5  \n",
       "35140         0.5   0.090909    0.523626       0.44        0.5  \n",
       "23546         0.0   0.181818    0.322139       0.36        0.5  \n",
       "546           0.1   0.363636    0.324756       0.20        0.5  \n",
       "\n",
       "[5 rows x 171 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce to 40 features\n",
    "encoding_dim = 40\n",
    "\n",
    "input_df = Input(shape=(171,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_df)\n",
    "decoded = Dense(171, activation='sigmoid')(encoded)\n",
    "\n",
    "# encoder\n",
    "autoencoder = Model(input_df, decoded)\n",
    "\n",
    "# intermediate result\n",
    "encoder = Model(input_df, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32836 samples, validate on 14073 samples\n",
      "Epoch 1/150\n",
      "32836/32836 [==============================] - 1s 25us/step - loss: 0.0384 - val_loss: 0.0272\n",
      "Epoch 2/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0255 - val_loss: 0.0248\n",
      "Epoch 3/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0247 - val_loss: 0.0245\n",
      "Epoch 4/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0244 - val_loss: 0.0243\n",
      "Epoch 5/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0242 - val_loss: 0.0241\n",
      "Epoch 6/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0240 - val_loss: 0.0239\n",
      "Epoch 7/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0238 - val_loss: 0.0237\n",
      "Epoch 8/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0236 - val_loss: 0.0235\n",
      "Epoch 9/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0234 - val_loss: 0.0233\n",
      "Epoch 10/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0232 - val_loss: 0.0230\n",
      "Epoch 11/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0229 - val_loss: 0.0228\n",
      "Epoch 12/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0227 - val_loss: 0.0225\n",
      "Epoch 13/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0224 - val_loss: 0.0222\n",
      "Epoch 14/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0221 - val_loss: 0.0219\n",
      "Epoch 15/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0218 - val_loss: 0.0216\n",
      "Epoch 16/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0215 - val_loss: 0.0213\n",
      "Epoch 17/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0212 - val_loss: 0.0210\n",
      "Epoch 18/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0209 - val_loss: 0.0207\n",
      "Epoch 19/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0206 - val_loss: 0.0204\n",
      "Epoch 20/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0203 - val_loss: 0.0201\n",
      "Epoch 21/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0200 - val_loss: 0.0198\n",
      "Epoch 22/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0197 - val_loss: 0.0195\n",
      "Epoch 23/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0194 - val_loss: 0.0192\n",
      "Epoch 24/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0192 - val_loss: 0.0190\n",
      "Epoch 25/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0189 - val_loss: 0.0187\n",
      "Epoch 26/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0186 - val_loss: 0.0184\n",
      "Epoch 27/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0184 - val_loss: 0.0182\n",
      "Epoch 28/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0181 - val_loss: 0.0179\n",
      "Epoch 29/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0179 - val_loss: 0.0177\n",
      "Epoch 30/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0176 - val_loss: 0.0174\n",
      "Epoch 31/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0174 - val_loss: 0.0172\n",
      "Epoch 32/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0171 - val_loss: 0.0170\n",
      "Epoch 33/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0169 - val_loss: 0.0167\n",
      "Epoch 34/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0167 - val_loss: 0.0165\n",
      "Epoch 35/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0165 - val_loss: 0.0163\n",
      "Epoch 36/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0163 - val_loss: 0.0161\n",
      "Epoch 37/150\n",
      "32836/32836 [==============================] - 1s 15us/step - loss: 0.0161 - val_loss: 0.0159\n",
      "Epoch 38/150\n",
      "32836/32836 [==============================] - 1s 17us/step - loss: 0.0159 - val_loss: 0.0158\n",
      "Epoch 39/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0157 - val_loss: 0.0156\n",
      "Epoch 40/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0155 - val_loss: 0.0154\n",
      "Epoch 41/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0154 - val_loss: 0.0153\n",
      "Epoch 42/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0152 - val_loss: 0.0151\n",
      "Epoch 43/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0151 - val_loss: 0.0150\n",
      "Epoch 44/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0150 - val_loss: 0.0149\n",
      "Epoch 45/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0148 - val_loss: 0.0148\n",
      "Epoch 46/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0147 - val_loss: 0.0146\n",
      "Epoch 47/150\n",
      "32836/32836 [==============================] - 1s 16us/step - loss: 0.0146 - val_loss: 0.0145\n",
      "Epoch 48/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0145 - val_loss: 0.0144\n",
      "Epoch 49/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0144 - val_loss: 0.0143\n",
      "Epoch 50/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0143 - val_loss: 0.0142\n",
      "Epoch 51/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0142 - val_loss: 0.0141\n",
      "Epoch 52/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 53/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0140 - val_loss: 0.0139\n",
      "Epoch 54/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0139 - val_loss: 0.0139\n",
      "Epoch 55/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0138 - val_loss: 0.0138\n",
      "Epoch 56/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0137 - val_loss: 0.0137\n",
      "Epoch 57/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0137 - val_loss: 0.0136\n",
      "Epoch 58/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0136 - val_loss: 0.0135\n",
      "Epoch 59/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0135 - val_loss: 0.0135\n",
      "Epoch 60/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0134 - val_loss: 0.0134\n",
      "Epoch 61/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0134 - val_loss: 0.0133\n",
      "Epoch 62/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0133 - val_loss: 0.0132\n",
      "Epoch 63/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0132 - val_loss: 0.0132\n",
      "Epoch 64/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0132 - val_loss: 0.0131\n",
      "Epoch 65/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0131 - val_loss: 0.0131\n",
      "Epoch 66/150\n",
      "32836/32836 [==============================] - 1s 16us/step - loss: 0.0130 - val_loss: 0.0130\n",
      "Epoch 67/150\n",
      "32836/32836 [==============================] - 1s 15us/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 68/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0129 - val_loss: 0.0129\n",
      "Epoch 69/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0129 - val_loss: 0.0128\n",
      "Epoch 70/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0128 - val_loss: 0.0128\n",
      "Epoch 71/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0127 - val_loss: 0.0127\n",
      "Epoch 72/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0127 - val_loss: 0.0126\n",
      "Epoch 73/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0126 - val_loss: 0.0126\n",
      "Epoch 74/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0126 - val_loss: 0.0125\n",
      "Epoch 75/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0125 - val_loss: 0.0125\n",
      "Epoch 76/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0125 - val_loss: 0.0124\n",
      "Epoch 77/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32836/32836 [==============================] - 1s 16us/step - loss: 0.0124 - val_loss: 0.0124\n",
      "Epoch 78/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0124 - val_loss: 0.0123\n",
      "Epoch 79/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0123 - val_loss: 0.0123\n",
      "Epoch 80/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0123 - val_loss: 0.0122\n",
      "Epoch 81/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0122 - val_loss: 0.0122\n",
      "Epoch 82/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0122 - val_loss: 0.0121\n",
      "Epoch 83/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0121 - val_loss: 0.0121\n",
      "Epoch 84/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0121 - val_loss: 0.0121\n",
      "Epoch 85/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0120 - val_loss: 0.0120\n",
      "Epoch 86/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0120 - val_loss: 0.0120\n",
      "Epoch 87/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0120 - val_loss: 0.0119\n",
      "Epoch 88/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 89/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 90/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 91/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 92/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 93/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 94/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0117 - val_loss: 0.0116\n",
      "Epoch 95/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0116 - val_loss: 0.0116\n",
      "Epoch 96/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0116 - val_loss: 0.0116\n",
      "Epoch 97/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0116 - val_loss: 0.0115\n",
      "Epoch 98/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 99/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 100/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 101/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 102/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 103/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 104/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 105/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 106/150\n",
      "32836/32836 [==============================] - 1s 15us/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 107/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 108/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 109/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 110/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 111/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 112/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 113/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 114/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 115/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 116/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 117/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 118/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 119/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 120/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 121/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 122/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 123/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 124/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 125/150\n",
      "32836/32836 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 126/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 127/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 128/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 129/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 130/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 131/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 132/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 133/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 134/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 135/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 136/150\n",
      "32836/32836 [==============================] - 1s 15us/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 137/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 138/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 139/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 140/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 141/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 142/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 143/150\n",
      "32836/32836 [==============================] - 0s 14us/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 144/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 145/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0102 - val_loss: 0.0101\n",
      "Epoch 146/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 147/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 148/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 149/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 150/150\n",
      "32836/32836 [==============================] - 0s 13us/step - loss: 0.0100 - val_loss: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb47ed66d8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=150,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_val, X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply prediction on reduced dimensional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_X_train = encoder.predict(X_train)\n",
    "encoded_X_val = encoder.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "[1]\tvalid_0's binary_logloss: 0.650816\n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.636275\n",
      "[3]\tvalid_0's binary_logloss: 0.626666\n",
      "[4]\tvalid_0's binary_logloss: 0.61973\n",
      "[5]\tvalid_0's binary_logloss: 0.614388\n",
      "[6]\tvalid_0's binary_logloss: 0.60939\n",
      "[7]\tvalid_0's binary_logloss: 0.6073\n",
      "[8]\tvalid_0's binary_logloss: 0.605893\n",
      "[9]\tvalid_0's binary_logloss: 0.60457\n",
      "[10]\tvalid_0's binary_logloss: 0.601996\n",
      "[11]\tvalid_0's binary_logloss: 0.601164\n",
      "[12]\tvalid_0's binary_logloss: 0.600583\n",
      "[13]\tvalid_0's binary_logloss: 0.60018\n",
      "[14]\tvalid_0's binary_logloss: 0.598705\n",
      "[15]\tvalid_0's binary_logloss: 0.598032\n",
      "[16]\tvalid_0's binary_logloss: 0.598207\n",
      "[17]\tvalid_0's binary_logloss: 0.597882\n",
      "[18]\tvalid_0's binary_logloss: 0.597881\n",
      "[19]\tvalid_0's binary_logloss: 0.598183\n",
      "[20]\tvalid_0's binary_logloss: 0.598251\n",
      "[21]\tvalid_0's binary_logloss: 0.598061\n",
      "[22]\tvalid_0's binary_logloss: 0.598631\n",
      "[23]\tvalid_0's binary_logloss: 0.598685\n",
      "[24]\tvalid_0's binary_logloss: 0.598927\n",
      "[25]\tvalid_0's binary_logloss: 0.599262\n",
      "[26]\tvalid_0's binary_logloss: 0.598939\n",
      "[27]\tvalid_0's binary_logloss: 0.599537\n",
      "[28]\tvalid_0's binary_logloss: 0.598855\n",
      "[29]\tvalid_0's binary_logloss: 0.598375\n",
      "[30]\tvalid_0's binary_logloss: 0.598313\n",
      "[31]\tvalid_0's binary_logloss: 0.599005\n",
      "[32]\tvalid_0's binary_logloss: 0.598606\n",
      "[33]\tvalid_0's binary_logloss: 0.598996\n",
      "[34]\tvalid_0's binary_logloss: 0.598412\n",
      "[35]\tvalid_0's binary_logloss: 0.598451\n",
      "[36]\tvalid_0's binary_logloss: 0.59922\n",
      "[37]\tvalid_0's binary_logloss: 0.599165\n",
      "[38]\tvalid_0's binary_logloss: 0.599444\n",
      "[39]\tvalid_0's binary_logloss: 0.599405\n",
      "[40]\tvalid_0's binary_logloss: 0.599275\n",
      "[41]\tvalid_0's binary_logloss: 0.599705\n",
      "[42]\tvalid_0's binary_logloss: 0.599506\n",
      "[43]\tvalid_0's binary_logloss: 0.599621\n",
      "[44]\tvalid_0's binary_logloss: 0.599615\n",
      "[45]\tvalid_0's binary_logloss: 0.599471\n",
      "[46]\tvalid_0's binary_logloss: 0.599558\n",
      "[47]\tvalid_0's binary_logloss: 0.599948\n",
      "[48]\tvalid_0's binary_logloss: 0.60029\n",
      "[49]\tvalid_0's binary_logloss: 0.600156\n",
      "[50]\tvalid_0's binary_logloss: 0.600803\n",
      "[51]\tvalid_0's binary_logloss: 0.601034\n",
      "[52]\tvalid_0's binary_logloss: 0.601044\n",
      "[53]\tvalid_0's binary_logloss: 0.601177\n",
      "[54]\tvalid_0's binary_logloss: 0.600894\n",
      "[55]\tvalid_0's binary_logloss: 0.600899\n",
      "[56]\tvalid_0's binary_logloss: 0.601094\n",
      "[57]\tvalid_0's binary_logloss: 0.600735\n",
      "[58]\tvalid_0's binary_logloss: 0.601282\n",
      "[59]\tvalid_0's binary_logloss: 0.601648\n",
      "[60]\tvalid_0's binary_logloss: 0.601895\n",
      "[61]\tvalid_0's binary_logloss: 0.602288\n",
      "[62]\tvalid_0's binary_logloss: 0.601991\n",
      "[63]\tvalid_0's binary_logloss: 0.60257\n",
      "[64]\tvalid_0's binary_logloss: 0.602926\n",
      "[65]\tvalid_0's binary_logloss: 0.603215\n",
      "[66]\tvalid_0's binary_logloss: 0.603016\n",
      "[67]\tvalid_0's binary_logloss: 0.603427\n",
      "[68]\tvalid_0's binary_logloss: 0.603272\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.597881\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(encoded_X_train, y_train, free_raw_data=False)\n",
    "lgb_test = lgb.Dataset(encoded_X_val, y_val, reference=lgb_train,  free_raw_data=False)\n",
    "\n",
    "params = {\"objective\": \"binary\",\n",
    "          \"metric\": {\"binary_logloss\"},\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": 0.01,\n",
    "          \"max_depth\": 4,\n",
    "          \"num_leaves\": 16,\n",
    "          \"min_data_in_leaf\": 30,\n",
    "          \"min_child_samples\": 10,\n",
    "          }\n",
    "\n",
    "print('start training...')\n",
    "\n",
    "model = lgb.train(params,\n",
    "                  lgb_train,\n",
    "                  num_boost_round=200,\n",
    "                  valid_sets=lgb_test,\n",
    "                  early_stopping_rounds=50,\n",
    "                  learning_rates=lambda iter: 0.7 * (0.999 ** iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
